## Hi there ğŸ‘‹

I am SmiteshğŸ‘‹, I am a software engineer with extensive experience in developing and optimizing APIs using Node.js, ExpressJS, 
and PostgreSQL. I have experience in building scalable data solutions and ETL pipelines using SQL and 
Informatica PowerCenter. Currently, I am working with Adaptempy
## About me 

- âœ’ï¸ As long as data is involved in a problem statement, I will dive deep to solve it!
- ğŸ”­ Iâ€™m currently working with Adaptemy on develoving adaptive learning solutions.
- ğŸŒ± Iâ€™m currently learning Azure services for data engineering and preparing for Azure Data Engineer Associate Exam.
- ğŸ“« How to reach me: [Email](smitesh22@gmail.com) / [Linkedin](https://www.linkedin.com/in/smitesh-patil/).


![](https://komarev.com/ghpvc/?username=smitesh22&color=blueviolet)

# General Idea on my pinned projects

## 1. Unsupervised-Machine-Learning-For-Solar-Site-Selection  :
### Tech Stack : [Python, QGIS, PyTorch, Numpy, Pandas, Searborn, Spacy, LaTEX, GeoPandas,API services for data]

- Utilized geospatial data to select optimal sites for solar energy projects, leveraging advanced deep learning technique.
- Analysed Geological Information Systems (GIS) data and developed a machine learning pipeline that involved preprocessing GIS data from multiple web databases, modeling the data, and staging it for input into a deep learning model.
- Developed a multi-input Auto-Encoder to learn representations from geospatial data and applied various clustering algorithms to cluster optimal solar locations.

## 2.  daft.ie_dataengineering_and_analysis
### Tech Stack: [Python, Apache Airflow, Alchemy for Sql, Docker, Airflow Scheduler]

- Orchestrated a data pipeline using Apache Airflow to ingest data from Daft.ie.
- Implemented an ELT process deployed on the Airflow web server, where the ingested data was stored on an AWS S3 bucket and transformed before being loaded into a Snowflake database, serving as the data lake.
- Created a star schema in the data transformation phase by normalising the input data, which was subsequently utilized for data visualization and exploratory data analysis (EDA) in Tableau.
- Automated the process by utilizing the Airflow Scheduler to monitor the AWS bucket for changes and containerized the project using Docker for easy deployment.

## 3.  Masters-Assignment-Data
### Tech Stack: [Python, R, PyTorch, Numpy, Pandas, Searborn, Sci-kit learn, LaTEX]

- Repository for my college work during my Master's at University of Galway.

  
